**Dataset**
===
**Reward Models Datasets**
__________________________
RMhelp training dataset is collected from the [PKU-SafeRLHF (helpfulness)](https://github.com/PKU-Alignment/beavertails), [Anthropic Helpful](https://github.com/anthropics/hh-rlhf),
and the [Standford SHP](https://huggingface.co/datasets/stanfordnlp/SHP).<br>
RMsafe training dataset is collected from the [PKU-SafeRLHF (safe)](https://github.com/PKU-Alignment/beavertails) and [Anthropic Harmless](https://github.com/anthropics/hh-rlhf).
<br><br>
**MOLMA Dataset**
_________________
MOLMA training dataset is given in the query_only_MODRL_dataset.json in the MOLMA directory. The dataset is comprised of unanswered prompts collected from the [Alpaca Cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned) and the [Anthropic Harmless](https://github.com/anthropics/hh-rlhf) datasets.<br>

**Performance Examples**
=============
