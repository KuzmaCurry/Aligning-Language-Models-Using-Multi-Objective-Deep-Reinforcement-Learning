**Dataset**
===
**Reward Models Datasets**
__________________________
RMhelp training dataset is collected from the [PKU-SafeRLHF (helpfulness)](https://github.com/PKU-Alignment/beavertails), [Anthropic Helpful](https://github.com/anthropics/hh-rlhf),
and the [Standford SHP](https://huggingface.co/datasets/stanfordnlp/SHP).<br>
RMsafe is collected from the [PKU-SafeRLHF (safe)](https://github.com/PKU-Alignment/beavertails) and [Anthropic Harmless](https://github.com/anthropics/hh-rlhf).
<br><br>
**MOLMA Dataset**
_________________
